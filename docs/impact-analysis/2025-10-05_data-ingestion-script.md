# 影響範囲分析：データ取得スクリプト

## 基本情報

- **日時**: 2025-10-05
- **担当者**: Claude Code
- **変更理由**: Google SheetsからSupabaseへのデータ投入パイプライン構築

---

## データ規模の確認

### 既知の情報
- **シート数**: 182シート（大規模）
- **1シートあたりのデータ量**: 約7000文字
- **総データ量**: 約 1.3MB（182 × 7000文字）
- **データ品質**: 不要なタグが散在、クリーニング必須

### 課題
1. **大規模データ**: 182シート × 7000文字 = 膨大なデータ量
2. **データの汚さ**: 不要なタグ、書式が混在
3. **API制限**: Google Sheets API、Gemini API、Supabaseのレート制限
4. **処理時間**: 全データ処理に相当な時間がかかる可能性

---

## 変更内容

### 変更対象ファイル
- [ ] `scripts/fetch-sheets-data.ts` - 新規作成（Google Sheets取得）
- [ ] `scripts/clean-data.ts` - 新規作成（データクリーニング）
- [ ] `scripts/chunk-data.ts` - 新規作成（チャンク分割）
- [ ] `scripts/embed-data.ts` - 新規作成（ベクトル化）
- [ ] `scripts/sync-to-supabase.ts` - 新規作成（統合スクリプト）
- [ ] `src/lib/google-sheets.ts` - 新規作成（Google Sheets クライアント）
- [ ] `src/lib/gemini.ts` - 新規作成（Gemini API クライアント）
- [ ] `src/lib/text-cleaner.ts` - 新規作成（テキストクリーニング）
- [ ] `src/lib/chunker.ts` - 新規作成（チャンク分割ロジック）
- [ ] `package.json` - 依存関係追加

### 実施内容
1. **データ取得**（Google Sheets API）
   - 182シートからデータ取得
   - バッチ処理（API制限対策）
   - リトライ機構

2. **データクリーニング**
   - HTMLタグ除去
   - 不要な書式除去
   - 特殊文字の正規化

3. **チャンク分割**
   - 意味のある単位で分割（対話単位）
   - 最大トークン数: 500トークン/チャンク
   - オーバーラップ: 前後の文脈を含める

4. **ベクトル化**（Gemini Embedding API）
   - text-embedding-004 使用（768次元）
   - バッチ処理
   - レート制限対策

5. **Supabaseへ投入**
   - バッチインサート
   - エラーハンドリング
   - 進捗表示

---

## 影響範囲分析

### 1. 直接的な影響

#### 新規作成ファイル
```
scripts/
├── fetch-sheets-data.ts
├── clean-data.ts
├── chunk-data.ts
├── embed-data.ts
└── sync-to-supabase.ts

src/lib/
├── google-sheets.ts
├── gemini.ts
├── text-cleaner.ts
└── chunker.ts
```

#### 変更ファイル
- `package.json` - googleapis, @google/generative-ai 追加

### 2. 間接的な影響

#### API使用量
- **Google Sheets API**: 約200リクエスト（シート数 + メタデータ）
- **Gemini Embedding API**: 推定 5000-10000リクエスト
  - 計算: 182シート × 7000文字 ÷ 500文字/チャンク = 約2500チャンク
  - バッチ処理で効率化
- **Supabase**: 約2500-5000レコード挿入

#### 処理時間
- **推定**: 30分 - 1時間（全データ処理）
- レート制限により変動

### 3. リスク評価

#### 高リスク
- [ ] Gemini API レート制限超過
  - 対策: バッチ処理、遅延挿入、リトライ機構
- [ ] メモリ不足（大量データ処理）
  - 対策: ストリーミング処理、チャンク単位での処理

#### 中リスク
- [ ] データクリーニングの精度
  - 対策: サンプルデータでテスト、段階的改善
- [ ] チャンク分割の最適化
  - 対策: 複数の分割戦略を試験

#### 低リスク
- [ ] Google Sheets API制限
  - 対策: 1日あたり十分な無料枠

---

## データクリーニング戦略

### 除去対象
1. **HTMLタグ**: `<p>`, `<br>`, `<span>` など
2. **書式タグ**: `<b>`, `<i>`, `<u>` など
3. **不要な空白**: 連続する空白、タブ
4. **特殊文字**: 制御文字、ゼロ幅文字

### 保持対象
- 対話の内容
- 質問と回答の構造
- 意味のある改行

### クリーニングロジック
```typescript
1. HTMLタグを除去
2. 連続する空白を1つに
3. 前後の空白をトリム
4. 特殊文字を正規化
```

---

## チャンク分割戦略

### 方針
- **単位**: 対話単位（質問 + 回答）
- **最大サイズ**: 500トークン
- **オーバーラップ**: 50トークン（前後の文脈）

### 分割ロジック
```
1. シート内の対話を抽出
2. 各対話を評価（トークン数）
3. 500トークン超える場合は分割
4. 前後の対話を文脈として付加
```

---

## テスト計画

### ステップ1: 小規模テスト
- [ ] 1シートのみでテスト
- [ ] データクリーニング結果を確認
- [ ] チャンク分割結果を確認
- [ ] ベクトル化・投入テスト

### ステップ2: 中規模テスト
- [ ] 10シートでテスト
- [ ] パフォーマンス確認
- [ ] エラーハンドリング確認

### ステップ3: 全データ投入
- [ ] 182シート全体を処理
- [ ] 進捗モニタリング
- [ ] エラーログ確認

---

## ユーザー確認事項

### 確認が必要なポイント
1. スプレッドシートの具体的なデータ構造を確認したい
   - E7列以降のデータ形式
   - タグの種類
   - 対話の区切り方
2. 処理時間が30分-1時間かかる可能性があるが問題ないか？
3. テストは小規模（1シート）から開始してよいか？

### 実施前の準備
- [ ] スプレッドシートのサンプルデータを確認
- [ ] データ構造を理解
- [ ] クリーニングロジックを設計

### ユーザー承認
- [ ] 影響範囲を確認し、実装を承認

---

## 実装ステップ

### ステップ1: データ構造調査
- [ ] 1シート（M6CH01001）のデータを手動確認
- [ ] データ形式を把握
- [ ] クリーニング戦略を具体化

### ステップ2: クリーニング・チャンキング実装
- [ ] text-cleaner.ts 実装
- [ ] chunker.ts 実装
- [ ] サンプルデータでテスト

### ステップ3: API統合
- [ ] Google Sheets クライアント実装
- [ ] Gemini クライアント実装
- [ ] レート制限対策

### ステップ4: 統合スクリプト
- [ ] sync-to-supabase.ts 実装
- [ ] 進捗表示
- [ ] エラーハンドリング

### ステップ5: テストと最適化
- [ ] 1シートテスト
- [ ] 10シートテスト
- [ ] 全データ投入

---

## 備考

- データ量が大きいため、段階的に進める
- まずサンプルデータで動作確認
- クリーニング精度は反復的に改善
